{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5af0cb82",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Lista de encabezados\n",
    "1. Piezas del Vehículo:\n",
    "    1: \"Antiniebla delantero derecho\",\n",
    "    2: \"Antiniebla delantero izquierdo\",\n",
    "    3: \"Capó\",\n",
    "    4: \"Cerradura capo\",\n",
    "    5: \"Cerradura maletero\",\n",
    "    6: \"Cerradura puerta\",\n",
    "    7: \"Espejo lateral derecho\",\n",
    "    8: \"Espejo lateral izquierdo\",\n",
    "    9: \"Faros derecho\",\n",
    "    10: \"Faros izquierdo\",\n",
    "    11: \"Guardabarros delantero derecho\",\n",
    "    12: \"Guardabarros delantero izquierdo\",\n",
    "    13: \"Guardabarros trasero derecho\",\n",
    "    14: \"Guardabarros trasero izquierdo\",\n",
    "    15: \"Luz indicadora delantera derecha\",\n",
    "    16: \"Luz indicadora delantera izquierda\",\n",
    "    17: \"Luz indicadora trasera derecha\",\n",
    "    18: \"Luz indicadora trasera izquierda\",\n",
    "    19: \"Luz trasera derecho\",\n",
    "    20: \"Luz trasera izquierdo\",\n",
    "    21: \"Maletero\",\n",
    "    22: \"Manija derecha\",\n",
    "    23: \"Manija izquierda\",\n",
    "    24: \"Marco de la ventana\",\n",
    "    25: \"Marco de las puertas\",\n",
    "    26: \"Moldura capó\",\n",
    "    27: \"Moldura maletro\",\n",
    "    28: \"Moldura puerta delantera derecha\",\n",
    "    29: \"Moldura puerta delantera izquierda\",\n",
    "    30: \"Moldura puerta trasera derecha\",\n",
    "    31: \"Moldura puerta trasera izquierda\",\n",
    "    32: \"Parabrisas delantero\",\n",
    "    33: \"Parabrisas trasero\",\n",
    "    34: \"Parachoques delantero\",\n",
    "    35: \"Parachoques trasero\",\n",
    "    36: \"Puerta delantera derecha\",\n",
    "    37: \"Puerta delantera izquierda\",\n",
    "    38: \"Puerta trasera derecha\",\n",
    "    39: \"Puerta trasera izquierda\",\n",
    "    40: \"Rejilla, parrilla\",\n",
    "    41: \"Rueda\",\n",
    "    42: \"Tapa de combustible\",\n",
    "    43: \"Tapa de rueda\",\n",
    "    44: \"Techo\",\n",
    "    45: \"Techo corredizo\",\n",
    "    46: \"Ventana delantera derecha\",\n",
    "    47: \"Ventana delantera izquierda\",\n",
    "    48: \"Ventana trasera derecha\",\n",
    "    49: \"Ventana trasera izquierda\",\n",
    "    50: \"Ventanilla delantera derecha\",\n",
    "    51: \"Ventanilla delantera izquierda\",\n",
    "    52: \"Ventanilla trasera derecha\",\n",
    "    53: \"Ventanilla trasera izquierda\"\n",
    "\n",
    "2. Tipos de Daño:\n",
    "    1: \"Abolladura\",\n",
    "    2: \"Deformación\",\n",
    "    3: \"Desprendimiento\",\n",
    "    4: \"Fractura\",\n",
    "    5: \"Rayón\",\n",
    "    6: \"Rotura\"\n",
    "\n",
    "3. Sugerencia:\n",
    "    1:\"Reparar\", \n",
    "    2:\"Reemplazar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90054558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# IMPORTS MEJORADOS\n",
    "# =============================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import os\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# =============================================\n",
    "# CONFIGURACIÓN MEJORADA\n",
    "# =============================================\n",
    "# Configuración adicional\n",
    "CLASS_WEIGHTS = True  \n",
    "FOCAL_LOSS = True     \n",
    "AUGMENTATION = True   \n",
    "EARLY_STOPPING = True \n",
    "USE_TENSORBOARD = True\n",
    "\n",
    "# Hiperparámetros optimizados\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 150  # Aumentado para permitir más aprendizaje\n",
    "MIN_SAMPLES_PER_CLASS = 20\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "PATIENCE = 20  # Aumentada para early stopping\n",
    "\n",
    "# Variables para guarda las métricas para graficar\n",
    "train_loss_history = []\n",
    "val_metric_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac204030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡TensorBoard importado correctamente!\n",
      "Prueba de escritura en TensorBoard completada\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    print(\"¡TensorBoard importado correctamente!\")\n",
    "    writer = SummaryWriter()\n",
    "    writer.add_scalar('test', 1.0, 1)\n",
    "    writer.close()\n",
    "    print(\"Prueba de escritura en TensorBoard completada\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4aa2092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# 1. DICCIONARIOS COMPLETOS DE MAPEO (CORREGIDOS)\n",
    "# =============================================\n",
    "label_to_cls_piezas = {\n",
    "    1: \"Antiniebla delantero derecho\",\n",
    "    2: \"Antiniebla delantero izquierdo\",\n",
    "    3: \"Capó\",\n",
    "    4: \"Cerradura capo\",\n",
    "    5: \"Cerradura maletero\",\n",
    "    6: \"Cerradura puerta\",\n",
    "    7: \"Espejo lateral derecho\",\n",
    "    8: \"Espejo lateral izquierdo\",\n",
    "    9: \"Faros derecho\",\n",
    "    10: \"Faros izquierdo\",\n",
    "    11: \"Guardabarros delantero derecho\",\n",
    "    12: \"Guardabarros delantero izquierdo\",\n",
    "    13: \"Guardabarros trasero derecho\",\n",
    "    14: \"Guardabarros trasero izquierdo\",\n",
    "    15: \"Luz indicadora delantera derecha\",\n",
    "    16: \"Luz indicadora delantera izquierda\",\n",
    "    17: \"Luz indicadora trasera derecha\",\n",
    "    18: \"Luz indicadora trasera izquierda\",\n",
    "    19: \"Luz trasera derecho\",\n",
    "    20: \"Luz trasera izquierdo\",\n",
    "    21: \"Maletero\",\n",
    "    22: \"Manija derecha\",\n",
    "    23: \"Manija izquierda\",\n",
    "    24: \"Marco de la ventana\",\n",
    "    25: \"Marco de las puertas\",\n",
    "    26: \"Moldura capó\",\n",
    "    27: \"Moldura puerta delantera derecha\",\n",
    "    28: \"Moldura puerta delantera izquierda\",\n",
    "    29: \"Moldura puerta trasera derecha\",\n",
    "    30: \"Moldura puerta trasera izquierda\",\n",
    "    31: \"Parabrisas delantero\",\n",
    "    32: \"Parabrisas trasero\",\n",
    "    33: \"Parachoques delantero\",\n",
    "    34: \"Parachoques trasero\",\n",
    "    35: \"Puerta delantera derecha\",\n",
    "    36: \"Puerta delantera izquierda\",\n",
    "    37: \"Puerta trasera derecha\",\n",
    "    38: \"Puerta trasera izquierda\",\n",
    "    39: \"Rejilla, parrilla\",\n",
    "    40: \"Rueda\",\n",
    "    41: \"Tapa de combustible\",\n",
    "    42: \"Tapa de rueda\",\n",
    "    43: \"Techo\",\n",
    "    44: \"Techo corredizo\",\n",
    "    45: \"Ventana delantera derecha\",\n",
    "    46: \"Ventana delantera izquierda\",\n",
    "    47: \"Ventana trasera derecha\",\n",
    "    48: \"Ventana trasera izquierda\",\n",
    "    49: \"Ventanilla delantera derecha\",\n",
    "    50: \"Ventanilla delantera izquierda\",\n",
    "    51: \"Ventanilla trasera derecha\",\n",
    "    52: \"Ventanilla trasera izquierda\"\n",
    "}\n",
    "\n",
    "# Diccionario para Tipos de Daño (completo)\n",
    "label_to_cls_danos = {\n",
    "    1: \"Abolladura\",\n",
    "    2: \"Deformación\",\n",
    "    3: \"Desprendimiento\",\n",
    "    4: \"Fractura\",\n",
    "    5: \"Rayón\",\n",
    "    6: \"Rotura\"\n",
    "}\n",
    "\n",
    "# Diccionario para Sugerencia (completo)\n",
    "label_to_cls_sugerencia = {\n",
    "    1: \"Reparar\",\n",
    "    2: \"Reemplazar\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9e8d298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# Cargar los datasets\n",
    "# =============================================\n",
    "multi_train = pd.read_csv('data/fotos_siniestros/datasets/multi_train.csv', sep='|')\n",
    "multi_val = pd.read_csv('data/fotos_siniestros/datasets/multi_val.csv', sep='|')\n",
    "multi_test = pd.read_csv('data/fotos_siniestros/datasets/multi_test.csv', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fdaa2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# Convertir string a lista\n",
    "# =============================================\n",
    "def convert_string_lists(df):\n",
    "    # Aplicar literal_eval a las columnas relevantes\n",
    "    df['partes'] = df['partes'].apply(ast.literal_eval)\n",
    "    df['dannos'] = df['dannos'].apply(ast.literal_eval)\n",
    "    df['sugerencias'] = df['sugerencias'].apply(ast.literal_eval)\n",
    "    return df\n",
    "\n",
    "# Aplicar a todos tus datasets\n",
    "multi_train = convert_string_lists(multi_train)\n",
    "multi_val = convert_string_lists(multi_val)\n",
    "multi_test = convert_string_lists(multi_test)\n",
    "\n",
    "# Rename columns to match dataset class expectations\n",
    "multi_train = multi_train[['Imagen', 'dannos', 'partes', 'sugerencias']].rename(columns={\n",
    "    'dannos': 'damages',\n",
    "    'partes': 'parts',\n",
    "    'sugerencias': 'suggestions'\n",
    "})\n",
    "\n",
    "# Rename columns to match dataset class expectations\n",
    "multi_val = multi_val[['Imagen', 'dannos', 'partes', 'sugerencias']].rename(columns={\n",
    "    'dannos': 'damages',\n",
    "    'partes': 'parts',\n",
    "    'sugerencias': 'suggestions'\n",
    "})\n",
    "\n",
    "multi_test = multi_test[['Imagen', 'dannos', 'partes', 'sugerencias']].rename(columns={\n",
    "    'dannos': 'damages',\n",
    "    'partes': 'parts',\n",
    "    'sugerencias': 'suggestions'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "046d5b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Imagen       damages            parts   suggestions\n",
      "0  563.jpg           [6]             [31]           [2]\n",
      "1  746.jpg  [1, 2, 2, 6]  [12, 33, 3, 10]  [1, 2, 1, 2]\n",
      "2  404.jpg        [1, 1]         [14, 38]        [1, 1]\n",
      "3  295.jpg           [5]             [33]           [1]\n",
      "4  817.jpg        [5, 5]         [33, 11]        [1, 1]\n",
      "    Imagen damages     parts suggestions\n",
      "0  130.jpg  [1, 1]  [36, 25]      [1, 1]\n",
      "1  437.jpg     [2]      [11]         [1]\n",
      "2  212.jpg     [6]      [47]         [2]\n",
      "3  338.jpg  [1, 2]  [25, 35]      [1, 1]\n",
      "4  326.jpg  [5, 6]  [13, 19]      [1, 2]\n",
      "    Imagen    damages        parts suggestions\n",
      "0  550.jpg        [4]         [34]         [1]\n",
      "1  469.jpg        [6]         [31]         [2]\n",
      "2  427.jpg        [5]         [12]         [1]\n",
      "3  842.jpg  [1, 1, 6]  [3, 43, 31]   [1, 1, 2]\n",
      "4  403.jpg     [1, 1]     [14, 38]      [1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(multi_train.head())\n",
    "print(multi_val.head())\n",
    "print(multi_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af113fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# MODELO\n",
    "# =============================================\n",
    "class MultiLabelDamageClassifier(nn.Module):\n",
    "    def __init__(self, num_parts, num_damages, num_suggestions):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Capa base (ResNet50 pre-entrenada)\n",
    "        self.base_model = models.resnet50(pretrained=True)\n",
    "        \n",
    "        # Congelar capas base inicialmente\n",
    "        for param in self.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Ajustar la última capa para nuestras tareas\n",
    "        num_features = self.base_model.fc.in_features\n",
    "        \n",
    "        # Cabezas de clasificación mejoradas\n",
    "        self.parts_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_parts)\n",
    "        )\n",
    "        \n",
    "        self.damages_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_damages)\n",
    "        )\n",
    "        \n",
    "        self.suggestions_head = nn.Sequential(\n",
    "            nn.Linear(num_features, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_suggestions)\n",
    "        )\n",
    "        \n",
    "        # Inicialización mejorada\n",
    "        for head in [self.parts_head, self.damages_head, self.suggestions_head]:\n",
    "            for m in head.modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.base_model.conv1(x)\n",
    "        features = self.base_model.bn1(features)\n",
    "        features = self.base_model.relu(features)\n",
    "        features = self.base_model.maxpool(features)\n",
    "        \n",
    "        features = self.base_model.layer1(features)\n",
    "        features = self.base_model.layer2(features)\n",
    "        features = self.base_model.layer3(features)\n",
    "        features = self.base_model.layer4(features)\n",
    "        \n",
    "        features = self.base_model.avgpool(features)\n",
    "        features = torch.flatten(features, 1)\n",
    "        \n",
    "        return {\n",
    "            'parts': self.parts_head(features),\n",
    "            'damages': self.damages_head(features),\n",
    "            'suggestions': self.suggestions_head(features)\n",
    "        }\n",
    "\n",
    "    def unfreeze_layers(self, num_layers=3):\n",
    "        \"\"\"Descongela capas para fine-tuning\"\"\"\n",
    "        for name, param in self.base_model.named_parameters():\n",
    "            if any(f'layer4.{i}.' in name for i in range(4)[-num_layers:]):\n",
    "                param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06ee935c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# DATASET\n",
    "# =============================================\n",
    "class BalancedMultiLabelDamageDataset(Dataset):\n",
    "    def __init__(self, dataframe, img_dir, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Convertir strings a listas\n",
    "        for col in ['parts', 'damages', 'suggestions']:\n",
    "            self.data[col] = self.data[col].apply(\n",
    "                lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "        \n",
    "        # Mapeo de clases\n",
    "        self.part_to_idx = {part: idx for idx, part in label_to_cls_piezas.items()}\n",
    "        self.damage_to_idx = {damage: idx for idx, damage in label_to_cls_danos.items()}\n",
    "        self.suggestion_to_idx = {sug: idx for idx, sug in label_to_cls_sugerencia.items()}\n",
    "        \n",
    "        # Binarizadores mejorados\n",
    "        self.part_binarizer = MultiLabelBinarizer(classes=sorted(self.part_to_idx.values()))\n",
    "        self.damage_binarizer = MultiLabelBinarizer(classes=sorted(self.damage_to_idx.values()))\n",
    "        self.suggestion_binarizer = MultiLabelBinarizer(classes=sorted(self.suggestion_to_idx.values()))\n",
    "        \n",
    "        # Calcular pesos mejorados\n",
    "        self.part_weights = self._calculate_weights('part')\n",
    "        self.damage_weights = self._calculate_weights('damage')\n",
    "        self.suggestion_weights = self._calculate_weights('suggestion')\n",
    "\n",
    "    def _calculate_weights(self, task):\n",
    "        \"\"\"Versión mejorada con square root inverse frequency\"\"\"\n",
    "        all_labels = [label for labels in self.data[f'{task}s'] \n",
    "                     for label in labels if label in getattr(self, f'{task}_to_idx')]\n",
    "        \n",
    "        if not all_labels:\n",
    "            return torch.ones(len(getattr(self, f'{task}_to_idx')), dtype=torch.float32).to(DEVICE)\n",
    "        \n",
    "        counts = Counter(all_labels)\n",
    "        total = len(all_labels)\n",
    "        weights = {cls: 1/np.sqrt(count/total) for cls, count in counts.items()}\n",
    "        \n",
    "        # Peso mínimo de 1.0 para clases no presentes\n",
    "        return torch.tensor([weights.get(cls, 1.0) for cls in sorted(getattr(self, f'{task}_to_idx').values())], \n",
    "                          dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = \"\"  # Inicializamos la variable\n",
    "        \n",
    "        try:\n",
    "            # Verificamos que el índice sea válido\n",
    "            if idx >= len(self.data):\n",
    "                raise IndexError(f\"Índice {idx} fuera de rango (tamaño del dataset: {len(self.data)})\")\n",
    "                \n",
    "            img_path = os.path.join(self.img_dir, self.data.iloc[idx]['Imagen'])\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error al cargar imagen (índice {idx}): {str(e)}\")\n",
    "            if not img_path:  # Si no se pudo obtener la ruta\n",
    "                img_path = f\"Índice inválido: {idx}\"\n",
    "            print(f\"Ruta problemática: {img_path}\")\n",
    "            image = torch.zeros(3, 224, 224)  # Imagen dummy\n",
    "        \n",
    "        # Resto del código para procesar etiquetas...\n",
    "        parts = torch.zeros(len(self.part_to_idx))\n",
    "        for part in self.data.iloc[idx]['parts']:\n",
    "            if part in self.part_to_idx:\n",
    "                parts[self.part_to_idx[part]] = 1\n",
    "                \n",
    "        damages = torch.zeros(len(self.damage_to_idx))\n",
    "        for damage in self.data.iloc[idx]['damages']:\n",
    "            if damage in self.damage_to_idx:\n",
    "                damages[self.damage_to_idx[damage]] = 1\n",
    "                \n",
    "        suggestions = torch.zeros(len(self.suggestion_to_idx))\n",
    "        for sug in self.data.iloc[idx]['suggestions']:\n",
    "            if sug in self.suggestion_to_idx:\n",
    "                suggestions[self.suggestion_to_idx[sug]] = 1\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, {\n",
    "            'parts': parts,\n",
    "            'damages': damages,\n",
    "            'suggestions': suggestions\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4de31b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# DATA AUGMENTATION\n",
    "# =============================================\n",
    "def get_transforms():\n",
    "    # Transformaciones base\n",
    "    base_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    if AUGMENTATION:\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.RandomAffine(0, shear=15),\n",
    "            transforms.RandomPerspective(distortion_scale=0.2, p=0.3),\n",
    "            transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.3),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    else:\n",
    "        train_transform = base_transform\n",
    "        \n",
    "    return {\n",
    "        'train': train_transform,\n",
    "        'val': base_transform,\n",
    "        'test': base_transform\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bdb1528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# EARLY STOPPING\n",
    "# =============================================\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        score = -val_loss\n",
    "        \n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c68db8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# Recolecta las imagenes\n",
    "# =============================================\n",
    "def collate_fn(batch):\n",
    "    # Filtrar None (imágenes que fallaron al cargar)\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    \n",
    "    # Si todo el batch falló, retornar un batch dummy\n",
    "    if len(batch) == 0:\n",
    "        dummy_image = torch.zeros(3, 224, 224)\n",
    "        dummy_target = {\n",
    "            'parts': torch.zeros(len(label_to_cls_piezas)),\n",
    "            'damages': torch.zeros(len(label_to_cls_danos)),\n",
    "            'suggestions': torch.zeros(len(label_to_cls_sugerencia))\n",
    "        }\n",
    "        return dummy_image.unsqueeze(0), dummy_target\n",
    "    \n",
    "    # Procesamiento normal\n",
    "    images = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "    \n",
    "    images = torch.stack(images, dim=0)\n",
    "    \n",
    "    batch_targets = {\n",
    "        'parts': torch.stack([t['parts'] for t in targets], dim=0),\n",
    "        'damages': torch.stack([t['damages'] for t in targets], dim=0),\n",
    "        'suggestions': torch.stack([t['suggestions'] for t in targets], dim=0)\n",
    "    }\n",
    "\n",
    "    return images, batch_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1638b20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# Evalua las metricas\n",
    "# =============================================\n",
    "def evaluate_multi_label(model, data_loader):\n",
    "    \"\"\"Evalúa el modelo en el conjunto de validación/test\"\"\"\n",
    "    model.eval()\n",
    "    parts_preds = []\n",
    "    parts_targets = []\n",
    "    damages_preds = []\n",
    "    damages_targets = []\n",
    "    suggestions_preds = []\n",
    "    suggestions_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Guardar predicciones y targets para cada tarea\n",
    "            parts_preds.append(torch.sigmoid(outputs['parts']).cpu())\n",
    "            parts_targets.append(targets['parts'].cpu())\n",
    "            \n",
    "            damages_preds.append(torch.sigmoid(outputs['damages']).cpu())\n",
    "            damages_targets.append(targets['damages'].cpu())\n",
    "            \n",
    "            suggestions_preds.append(torch.softmax(outputs['suggestions'], dim=1).cpu())\n",
    "            suggestions_targets.append(targets['suggestions'].cpu())\n",
    "    \n",
    "    # Concatenar todos los batches\n",
    "    parts_preds = torch.cat(parts_preds)\n",
    "    parts_targets = torch.cat(parts_targets)\n",
    "    damages_preds = torch.cat(damages_preds)\n",
    "    damages_targets = torch.cat(damages_targets)\n",
    "    suggestions_preds = torch.cat(suggestions_preds)\n",
    "    suggestions_targets = torch.cat(suggestions_targets)\n",
    "    \n",
    "    # Calcular métricas para cada tarea\n",
    "    def calculate_metrics(preds, targets, task_type='multilabel'):\n",
    "        if task_type == 'multilabel':\n",
    "            # Para partes y daños (clasificación multi-etiqueta)\n",
    "            preds = (preds > 0.5).float()  # Umbral de 0.5\n",
    "            accuracy = accuracy_score(targets, preds)\n",
    "            f1 = f1_score(targets, preds, average='macro')\n",
    "        else:\n",
    "            # Para sugerencias (clasificación multi-clase)\n",
    "            preds = preds.argmax(dim=1)\n",
    "            targets = targets.argmax(dim=1)\n",
    "            accuracy = accuracy_score(targets, preds)\n",
    "            f1 = f1_score(targets, preds, average='macro')\n",
    "        \n",
    "        return {'accuracy': accuracy, 'f1_macro': f1}\n",
    "    \n",
    "    metrics = {\n",
    "        'parts': calculate_metrics(parts_preds, parts_targets, 'multilabel'),\n",
    "        'damages': calculate_metrics(damages_preds, damages_targets, 'multilabel'),\n",
    "        'suggestions': calculate_metrics(suggestions_preds, suggestions_targets, 'multiclass')\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2373d980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# FUNCIONES DE PÉRDIDA\n",
    "# =============================================\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets, weights=None):\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "        \n",
    "        if weights is not None:\n",
    "            # Aplicar pesos por clase\n",
    "            class_weights = weights.expand_as(targets)\n",
    "            F_loss = F_loss * class_weights\n",
    "            \n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(F_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(F_loss)\n",
    "        return F_loss\n",
    "\n",
    "def balanced_multi_label_loss(outputs, targets, weights):\n",
    "    # Configuración mejorada de pérdidas\n",
    "    parts_loss_fn = FocalLoss(alpha=0.25, gamma=2.0) if FOCAL_LOSS else nn.BCEWithLogitsLoss()\n",
    "    damages_loss_fn = FocalLoss(alpha=0.5, gamma=2.0) if FOCAL_LOSS else nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    # Pérdidas con pesos\n",
    "    parts_loss = parts_loss_fn(\n",
    "        outputs['parts'], \n",
    "        targets['parts'].float(),\n",
    "        weights=weights['parts'] if CLASS_WEIGHTS else None\n",
    "    )\n",
    "    \n",
    "    damages_loss = damages_loss_fn(\n",
    "        outputs['damages'], \n",
    "        targets['damages'].float(),\n",
    "        weights=weights['damages'] if CLASS_WEIGHTS else None\n",
    "    )\n",
    "    \n",
    "    # Sugerencias con label smoothing\n",
    "    suggestions_loss = F.cross_entropy(\n",
    "        outputs['suggestions'],\n",
    "        targets['suggestions'].argmax(dim=1),\n",
    "        weight=weights['suggestions'] if CLASS_WEIGHTS else None,\n",
    "        label_smoothing=0.1\n",
    "    )\n",
    "    \n",
    "    return 0.4 * parts_loss + 0.4 * damages_loss + 0.2 * suggestions_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00e61f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# ENTRENAMIENTO\n",
    "# =============================================\n",
    "def train_model():\n",
    "    # Inicializar TensorBoard\n",
    "    writer = SummaryWriter() if USE_TENSORBOARD else None\n",
    "    \n",
    "    # Cargar transforms\n",
    "    data_transforms = get_transforms()\n",
    "    \n",
    "    # Crear datasets\n",
    "    train_dataset = BalancedMultiLabelDamageDataset(multi_train, '../data/fotos_siniestros/', data_transforms['train'])\n",
    "    val_dataset = BalancedMultiLabelDamageDataset(multi_val, '../data/fotos_siniestros/', data_transforms['val'])\n",
    "    \n",
    "    # Sampler mejorado\n",
    "    def get_sampler(dataset):\n",
    "        \"\"\"Sampler que considera múltiples tareas\"\"\"\n",
    "        labels = []\n",
    "        for idx in range(len(dataset)):\n",
    "            # Combinar etiquetas de partes y daños para balanceo\n",
    "            combined = tuple(dataset[idx][1]['parts'].nonzero().flatten().tolist() + \n",
    "                           dataset[idx][1]['damages'].nonzero().flatten().tolist())\n",
    "            labels.append(combined)\n",
    "        \n",
    "        class_counts = Counter(labels)\n",
    "        class_weights = {cls: 1./count for cls, count in class_counts.items()}\n",
    "        sample_weights = [class_weights[cls] for cls in labels]\n",
    "        \n",
    "        return WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
    "    \n",
    "    # DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        sampler=get_sampler(train_dataset),\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Modelo\n",
    "    model = MultiLabelDamageClassifier(\n",
    "        num_parts=len(label_to_cls_piezas),\n",
    "        num_damages=len(label_to_cls_danos),\n",
    "        num_suggestions=len(label_to_cls_sugerencia)\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    # Optimizador y scheduler mejorados\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=LR*10, \n",
    "        epochs=NUM_EPOCHS, \n",
    "        steps_per_epoch=len(train_loader),\n",
    "        pct_start=0.1\n",
    "    )\n",
    "    \n",
    "    # Early Stopping mejorado\n",
    "    early_stopper = EarlyStopping(patience=PATIENCE, delta=0.001) if EARLY_STOPPING else None\n",
    "    \n",
    "    # Entrenamiento\n",
    "    best_metric = -float('inf')\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        # Fine-tuning después de 20 épocas\n",
    "        if epoch == 20:\n",
    "            model.unfreeze_layers(2)\n",
    "        \n",
    "        # Fase de entrenamiento\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS}', leave=False)\n",
    "        \n",
    "        for inputs, targets in progress_bar:\n",
    "            inputs = inputs.to(DEVICE, non_blocking=True)\n",
    "            targets = {k: v.to(DEVICE, non_blocking=True) for k, v in targets.items()}\n",
    "            \n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = balanced_multi_label_loss(outputs, targets, {\n",
    "                'parts': train_dataset.part_weights,\n",
    "                'damages': train_dataset.damage_weights,\n",
    "                'suggestions': train_dataset.suggestion_weights\n",
    "            })\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "        \n",
    "        # Validación\n",
    "        val_metrics = evaluate_multi_label(model, val_loader)\n",
    "        current_metric = 0.4*val_metrics['parts']['f1_macro'] + 0.4*val_metrics['damages']['f1_macro'] + 0.2*val_metrics['suggestions']['f1_macro']\n",
    "        \n",
    "        # Early Stopping\n",
    "        if EARLY_STOPPING:\n",
    "            early_stopper(current_metric)\n",
    "            if early_stopper.early_stop:\n",
    "                print(f\"\\nEarly stopping at epoch {epoch+1} with metric {current_metric:.4f}\")\n",
    "                break\n",
    "        \n",
    "        # Guardar mejor modelo\n",
    "        if current_metric > best_metric:\n",
    "            best_metric = current_metric\n",
    "            torch.save(model.state_dict(), 'best_model_multilabel.pth')\n",
    "            print(f\"\\nNew best model saved with metric {best_metric:.4f}!\")\n",
    "        \n",
    "        # Logging mejorado\n",
    "        print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "        print(f\"Train Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "        print(f\"Validation Metric: {current_metric:.4f} (Best: {best_metric:.4f})\")\n",
    "        print(\"Detailed Metrics:\")\n",
    "        for task, metrics in val_metrics.items():\n",
    "            print(f\"  {task:12} - Acc: {metrics['accuracy']:.4f} | F1: {metrics['f1_macro']:.4f}\")\n",
    "        \n",
    "        # TensorBoard logging\n",
    "        if writer:\n",
    "            writer.add_scalar('Loss/train', epoch_loss/len(train_loader), epoch)\n",
    "            writer.add_scalar('Metric/val', current_metric, epoch)\n",
    "            for task, metrics in val_metrics.items():\n",
    "                writer.add_scalar(f'Accuracy/{task}', metrics['accuracy'], epoch)\n",
    "                writer.add_scalar(f'F1/{task}', metrics['f1_macro'], epoch)\n",
    "\n",
    "        # Guardas las métricas para graficar\n",
    "        train_loss_history.append(epoch_loss/len(train_loader))\n",
    "        val_metric_history.append(val_metrics)\n",
    "    \n",
    "    if writer:\n",
    "        writer.close()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26283c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# Verificar que todas las imágenes sean accesibles\n",
    "# =============================================\n",
    "def verify_dataset(dataset):\n",
    "    problematic_indices = []\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        try:\n",
    "            img, targets = dataset[i]\n",
    "            if torch.all(img == 0):  # Imagen dummy\n",
    "                problematic_indices.append(i)\n",
    "        except Exception as e:\n",
    "            problematic_indices.append(i)\n",
    "    \n",
    "    if problematic_indices:\n",
    "        print(f\"\\n¡Advertencia! {len(problematic_indices)} imágenes problemáticas encontradas\")\n",
    "        print(\"Primeros 10 índices problemáticos:\", problematic_indices[:10])\n",
    "    else:\n",
    "        print(\"\\n¡Dataset verificado correctamente!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18772298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying validation dataset:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 629/629 [00:06<00:00, 91.96it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "¡Dataset verificado correctamente!\n",
      "Verifying validation dataset:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 287/287 [00:03<00:00, 94.43it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "¡Dataset verificado correctamente!\n",
      "Verifying test dataset:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229/229 [00:02<00:00, 103.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "¡Dataset verificado correctamente!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load transforms\n",
    "data_transforms = get_transforms()\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = BalancedMultiLabelDamageDataset(multi_train, '../data/fotos_siniestros/', data_transforms['train'])\n",
    "val_dataset = BalancedMultiLabelDamageDataset(multi_val, '../data/fotos_siniestros/', data_transforms['val'])\n",
    "test_dataset = BalancedMultiLabelDamageDataset(multi_test, '../data/fotos_siniestros/', data_transforms['test'])\n",
    "\n",
    "# Verify datasets\n",
    "print(\"Verifying validation dataset:\")\n",
    "verify_dataset(train_dataset)\n",
    "\n",
    "print(\"Verifying validation dataset:\")\n",
    "verify_dataset(val_dataset)\n",
    "\n",
    "print(\"Verifying test dataset:\")\n",
    "verify_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2b61c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Python/VerificacionDeSiniestrosUsandoMultiplesEtiquetas/.venv/lib64/python3.13/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/data/Python/VerificacionDeSiniestrosUsandoMultiplesEtiquetas/.venv/lib64/python3.13/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best model saved with metric 0.0383!\n",
      "\n",
      "Epoch 1/150\n",
      "Train Loss: 1.7380\n",
      "Validation Metric: 0.0383 (Best: 0.0383)\n",
      "Detailed Metrics:\n",
      "  parts        - Acc: 0.0000 | F1: 0.0000\n",
      "  damages      - Acc: 0.0000 | F1: 0.0000\n",
      "  suggestions  - Acc: 0.2369 | F1: 0.1915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best model saved with metric 0.0590!\n",
      "\n",
      "Epoch 2/150\n",
      "Train Loss: 1.6570\n",
      "Validation Metric: 0.0590 (Best: 0.0590)\n",
      "Detailed Metrics:\n",
      "  parts        - Acc: 0.0000 | F1: 0.0000\n",
      "  damages      - Acc: 0.0000 | F1: 0.0000\n",
      "  suggestions  - Acc: 0.4181 | F1: 0.2948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best model saved with metric 0.0689!\n",
      "\n",
      "Epoch 3/150\n",
      "Train Loss: 1.5481\n",
      "Validation Metric: 0.0689 (Best: 0.0689)\n",
      "Detailed Metrics:\n",
      "  parts        - Acc: 0.0000 | F1: 0.0000\n",
      "  damages      - Acc: 0.0000 | F1: 0.0000\n",
      "  suggestions  - Acc: 0.5261 | F1: 0.3447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best model saved with metric 0.0807!\n",
      "\n",
      "Epoch 4/150\n",
      "Train Loss: 1.4355\n",
      "Validation Metric: 0.0807 (Best: 0.0807)\n",
      "Detailed Metrics:\n",
      "  parts        - Acc: 0.0000 | F1: 0.0000\n",
      "  damages      - Acc: 0.0035 | F1: 0.0000\n",
      "  suggestions  - Acc: 0.6760 | F1: 0.4033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best model saved with metric 0.0847!\n",
      "\n",
      "Epoch 5/150\n",
      "Train Loss: 1.3900\n",
      "Validation Metric: 0.0847 (Best: 0.0847)\n",
      "Detailed Metrics:\n",
      "  parts        - Acc: 0.0000 | F1: 0.0000\n",
      "  damages      - Acc: 0.0035 | F1: 0.0000\n",
      "  suggestions  - Acc: 0.7352 | F1: 0.4237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Python/VerificacionDeSiniestrosUsandoMultiplesEtiquetas/.venv/lib64/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best model saved with metric 0.0863!\n",
      "\n",
      "Epoch 6/150\n",
      "Train Loss: 1.2578\n",
      "Validation Metric: 0.0863 (Best: 0.0863)\n",
      "Detailed Metrics:\n",
      "  parts        - Acc: 0.0000 | F1: 0.0000\n",
      "  damages      - Acc: 0.0070 | F1: 0.0000\n",
      "  suggestions  - Acc: 0.7596 | F1: 0.4317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Python/VerificacionDeSiniestrosUsandoMultiplesEtiquetas/.venv/lib64/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best model saved with metric 0.0907!\n",
      "\n",
      "Epoch 7/150\n",
      "Train Loss: 1.0935\n",
      "Validation Metric: 0.0907 (Best: 0.0907)\n",
      "Detailed Metrics:\n",
      "  parts        - Acc: 0.0000 | F1: 0.0000\n",
      "  damages      - Acc: 0.0105 | F1: 0.0000\n",
      "  suggestions  - Acc: 0.8293 | F1: 0.4533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Python/VerificacionDeSiniestrosUsandoMultiplesEtiquetas/.venv/lib64/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/150\n",
      "Train Loss: 0.9643\n",
      "Validation Metric: 0.0847 (Best: 0.0907)\n",
      "Detailed Metrics:\n",
      "  parts        - Acc: 0.0000 | F1: 0.0000\n",
      "  damages      - Acc: 0.0348 | F1: 0.0000\n",
      "  suggestions  - Acc: 0.7352 | F1: 0.4237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Python/VerificacionDeSiniestrosUsandoMultiplesEtiquetas/.venv/lib64/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/150\n",
      "Train Loss: 0.7470\n",
      "Validation Metric: 0.0892 (Best: 0.0907)\n",
      "Detailed Metrics:\n",
      "  parts        - Acc: 0.0557 | F1: 0.0000\n",
      "  damages      - Acc: 0.0418 | F1: 0.0000\n",
      "  suggestions  - Acc: 0.8049 | F1: 0.4459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Python/VerificacionDeSiniestrosUsandoMultiplesEtiquetas/.venv/lib64/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/150\n",
      "Train Loss: 0.6766\n",
      "Validation Metric: 0.0894 (Best: 0.0907)\n",
      "Detailed Metrics:\n",
      "  parts        - Acc: 0.2230 | F1: 0.0000\n",
      "  damages      - Acc: 0.0732 | F1: 0.0000\n",
      "  suggestions  - Acc: 0.8084 | F1: 0.4470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Python/VerificacionDeSiniestrosUsandoMultiplesEtiquetas/.venv/lib64/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best model saved with metric 0.0962!\n",
      "\n",
      "Epoch 11/150\n",
      "Train Loss: 0.5715\n",
      "Validation Metric: 0.0962 (Best: 0.0962)\n",
      "Detailed Metrics:\n",
      "  parts        - Acc: 0.4355 | F1: 0.0000\n",
      "  damages      - Acc: 0.1777 | F1: 0.0000\n",
      "  suggestions  - Acc: 0.9268 | F1: 0.4810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Python/VerificacionDeSiniestrosUsandoMultiplesEtiquetas/.venv/lib64/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best model saved with metric 0.0998!\n",
      "\n",
      "Epoch 12/150\n",
      "Train Loss: 0.4808\n",
      "Validation Metric: 0.0998 (Best: 0.0998)\n",
      "Detailed Metrics:\n",
      "  parts        - Acc: 0.6655 | F1: 0.0000\n",
      "  damages      - Acc: 0.1463 | F1: 0.0000\n",
      "  suggestions  - Acc: 0.9965 | F1: 0.4991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Python/VerificacionDeSiniestrosUsandoMultiplesEtiquetas/.venv/lib64/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/data/Python/VerificacionDeSiniestrosUsandoMultiplesEtiquetas/.venv/lib64/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/150\n",
      "Train Loss: 0.3897\n",
      "Validation Metric: 0.0905 (Best: 0.0998)\n",
      "Detailed Metrics:\n",
      "  parts        - Acc: 0.8014 | F1: 0.0000\n",
      "  damages      - Acc: 0.1986 | F1: 0.0000\n",
      "  suggestions  - Acc: 0.8258 | F1: 0.4523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Python/VerificacionDeSiniestrosUsandoMultiplesEtiquetas/.venv/lib64/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/150\n",
      "Train Loss: 0.3635\n",
      "Validation Metric: 0.0973 (Best: 0.0998)\n",
      "Detailed Metrics:\n",
      "  parts        - Acc: 0.8537 | F1: 0.0000\n",
      "  damages      - Acc: 0.4878 | F1: 0.0000\n",
      "  suggestions  - Acc: 0.9477 | F1: 0.4866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Python/VerificacionDeSiniestrosUsandoMultiplesEtiquetas/.venv/lib64/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/150\n",
      "Train Loss: 0.3259\n",
      "Validation Metric: 0.0988 (Best: 0.0998)\n",
      "Detailed Metrics:\n",
      "  parts        - Acc: 0.8850 | F1: 0.0000\n",
      "  damages      - Acc: 0.6307 | F1: 0.0000\n",
      "  suggestions  - Acc: 0.9756 | F1: 0.4938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Python/VerificacionDeSiniestrosUsandoMultiplesEtiquetas/.venv/lib64/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/data/Python/VerificacionDeSiniestrosUsandoMultiplesEtiquetas/.venv/lib64/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best model saved with metric 0.2000!\n",
      "\n",
      "Epoch 16/150\n",
      "Train Loss: 0.2892\n",
      "Validation Metric: 0.2000 (Best: 0.2000)\n",
      "Detailed Metrics:\n",
      "  parts        - Acc: 0.9059 | F1: 0.0000\n",
      "  damages      - Acc: 0.6551 | F1: 0.0000\n",
      "  suggestions  - Acc: 1.0000 | F1: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Python/VerificacionDeSiniestrosUsandoMultiplesEtiquetas/.venv/lib64/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/data/Python/VerificacionDeSiniestrosUsandoMultiplesEtiquetas/.venv/lib64/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/150\n",
      "Train Loss: 0.2713\n",
      "Validation Metric: 0.0997 (Best: 0.2000)\n",
      "Detailed Metrics:\n",
      "  parts        - Acc: 0.9582 | F1: 0.0000\n",
      "  damages      - Acc: 0.7213 | F1: 0.0000\n",
      "  suggestions  - Acc: 0.9930 | F1: 0.4983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Python/VerificacionDeSiniestrosUsandoMultiplesEtiquetas/.venv/lib64/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/data/Python/VerificacionDeSiniestrosUsandoMultiplesEtiquetas/.venv/lib64/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18/150\n",
      "Train Loss: 0.2553\n",
      "Validation Metric: 0.0964 (Best: 0.2000)\n",
      "Detailed Metrics:\n",
      "  parts        - Acc: 0.9686 | F1: 0.0000\n",
      "  damages      - Acc: 0.6969 | F1: 0.0000\n",
      "  suggestions  - Acc: 0.9303 | F1: 0.4819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Python/VerificacionDeSiniestrosUsandoMultiplesEtiquetas/.venv/lib64/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/data/Python/VerificacionDeSiniestrosUsandoMultiplesEtiquetas/.venv/lib64/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19/150\n",
      "Train Loss: 0.2461\n",
      "Validation Metric: 0.0995 (Best: 0.2000)\n",
      "Detailed Metrics:\n",
      "  parts        - Acc: 0.9791 | F1: 0.0000\n",
      "  damages      - Acc: 0.7143 | F1: 0.0000\n",
      "  suggestions  - Acc: 0.9895 | F1: 0.4974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Python/VerificacionDeSiniestrosUsandoMultiplesEtiquetas/.venv/lib64/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/data/Python/VerificacionDeSiniestrosUsandoMultiplesEtiquetas/.venv/lib64/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20/150\n",
      "Train Loss: 0.2313\n",
      "Validation Metric: 0.0986 (Best: 0.2000)\n",
      "Detailed Metrics:\n",
      "  parts        - Acc: 0.9895 | F1: 0.0000\n",
      "  damages      - Acc: 0.8711 | F1: 0.0000\n",
      "  suggestions  - Acc: 0.9721 | F1: 0.4929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping at epoch 21 with metric 0.2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Python/VerificacionDeSiniestrosUsandoMultiplesEtiquetas/.venv/lib64/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/data/Python/VerificacionDeSiniestrosUsandoMultiplesEtiquetas/.venv/lib64/python3.13/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar entrenamiento\n",
    "\n",
    "# New best model saved with metric 0.0383!\n",
    "\n",
    "# Epoch 1/150\n",
    "# Train Loss: 1.7380\n",
    "# Validation Metric: 0.0383 (Best: 0.0383)\n",
    "# Detailed Metrics:\n",
    "#   parts        - Acc: 0.0000 | F1: 0.0000\n",
    "#   damages      - Acc: 0.0000 | F1: 0.0000\n",
    "#   suggestions  - Acc: 0.2369 | F1: 0.1915\n",
    "\n",
    "# Epoch 20/150\n",
    "# Train Loss: 0.2313\n",
    "# Validation Metric: 0.0986 (Best: 0.2000)\n",
    "# Detailed Metrics:\n",
    "#   parts        - Acc: 0.9895 | F1: 0.0000\n",
    "#   damages      - Acc: 0.8711 | F1: 0.0000\n",
    "#   suggestions  - Acc: 0.9721 | F1: 0.4929\n",
    "\n",
    "# Early stopping at epoch 21 with metric 0.2000\n",
    "\n",
    "# Last Execution 3:50:48 PM\n",
    "# Execution Time 21m 9.7s\n",
    "# Overhead Time 6m 11.8s\n",
    "# Render Times\n",
    "# VS Code Builtin Notebook Output Renderer 69ms\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    trained_model = train_model()\n",
    "    torch.save(trained_model.state_dict(), 'DetectarDannosPartesSugerenciasUsandoMultiplesEtiquetas_V5.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
