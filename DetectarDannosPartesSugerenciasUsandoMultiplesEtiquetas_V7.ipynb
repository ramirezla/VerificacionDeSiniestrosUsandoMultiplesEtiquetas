{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d87706ef",
   "metadata": {},
   "source": [
    "### Cambios Clave Implementados:\n",
    "\n",
    "1. Preprocesamiento de Datos:\n",
    "    - Agrupación de partes raras en categoría \"Otras\"\n",
    "    - Sobremuestreo de daños poco frecuentes\n",
    "\n",
    "2. Arquitectura del Modelo:\n",
    "    - Capas compartidas mejoradas\n",
    "    - Cabezales más profundos\n",
    "    - Descongelamiento progresivo\n",
    "\n",
    "3. Función de Pérdida:\n",
    "    - Focal Loss adaptativo con balanceo dinámico\n",
    "    - Pesos de clase mejor calculados\n",
    "\n",
    "4. Estrategia de Entrenamiento:\n",
    "    - Learning rates diferenciados\n",
    "    - Sampler balanceado para sugerencias\n",
    "    - Evaluación con umbrales adaptativos\n",
    "\n",
    "5. Monitoreo Mejorado:\n",
    "    - Evaluación detallada por clase\n",
    "    - Métricas más informativas\n",
    "    - Early stopping inteligente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a835bc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# IMPORTS (se mantienen igual)\n",
    "# =============================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import os\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# =============================================\n",
    "# CONFIGURACIÓN\n",
    "# =============================================\n",
    "# Nuevos parámetros añadidos\n",
    "CLASS_GROUPING = True  # Agrupar clases raras\n",
    "BALANCE_DAMAGES = True # Balancear daños raros\n",
    "MIN_SAMPLES_CLASS = 5  # Mínimo de muestras por clase\n",
    "\n",
    "# Configuración adicional\n",
    "CLASS_WEIGHTS = True  \n",
    "FOCAL_LOSS = True     \n",
    "AUGMENTATION = True   \n",
    "EARLY_STOPPING = True \n",
    "USE_TENSORBOARD = True\n",
    "\n",
    "# Hiperparámetros optimizados\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 100  # Aumentado para permitir más aprendizaje\n",
    "MIN_SAMPLES_PER_CLASS = 20\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "PATIENCE = 20  # Aumentada para early stopping\n",
    "\n",
    "# Variables para guarda las métricas para graficar\n",
    "train_loss_history = []\n",
    "val_metric_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39ad9cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# DICCIONARIOS COMPLETOS DE MAPEO\n",
    "# =============================================\n",
    "label_to_cls_piezas = {\n",
    "    1: \"Antiniebla delantero derecho\",\n",
    "    2: \"Antiniebla delantero izquierdo\",\n",
    "    3: \"Capó\",\n",
    "    4: \"Cerradura capo\",\n",
    "    5: \"Cerradura maletero\",\n",
    "    6: \"Cerradura puerta\",\n",
    "    7: \"Espejo lateral derecho\",\n",
    "    8: \"Espejo lateral izquierdo\",\n",
    "    9: \"Faros derecho\",\n",
    "    10: \"Faros izquierdo\",\n",
    "    11: \"Guardabarros delantero derecho\",\n",
    "    12: \"Guardabarros delantero izquierdo\",\n",
    "    13: \"Guardabarros trasero derecho\",\n",
    "    14: \"Guardabarros trasero izquierdo\",\n",
    "    15: \"Luz indicadora delantera derecha\",\n",
    "    16: \"Luz indicadora delantera izquierda\",\n",
    "    17: \"Luz indicadora trasera derecha\",\n",
    "    18: \"Luz indicadora trasera izquierda\",\n",
    "    19: \"Luz trasera derecho\",\n",
    "    20: \"Luz trasera izquierdo\",\n",
    "    21: \"Maletero\",\n",
    "    22: \"Manija derecha\",\n",
    "    23: \"Manija izquierda\",\n",
    "    24: \"Marco de la ventana\",\n",
    "    25: \"Marco de las puertas\",\n",
    "    26: \"Moldura capó\",\n",
    "    27: \"Moldura puerta delantera derecha\",\n",
    "    28: \"Moldura puerta delantera izquierda\",\n",
    "    29: \"Moldura puerta trasera derecha\",\n",
    "    30: \"Moldura puerta trasera izquierda\",\n",
    "    31: \"Parabrisas delantero\",\n",
    "    32: \"Parabrisas trasero\",\n",
    "    33: \"Parachoques delantero\",\n",
    "    34: \"Parachoques trasero\",\n",
    "    35: \"Puerta delantera derecha\",\n",
    "    36: \"Puerta delantera izquierda\",\n",
    "    37: \"Puerta trasera derecha\",\n",
    "    38: \"Puerta trasera izquierda\",\n",
    "    39: \"Rejilla, parrilla\",\n",
    "    40: \"Rueda\",\n",
    "    41: \"Tapa de combustible\",\n",
    "    42: \"Tapa de rueda\",\n",
    "    43: \"Techo\",\n",
    "    44: \"Techo corredizo\",\n",
    "    45: \"Ventana delantera derecha\",\n",
    "    46: \"Ventana delantera izquierda\",\n",
    "    47: \"Ventana trasera derecha\",\n",
    "    48: \"Ventana trasera izquierda\",\n",
    "    49: \"Ventanilla delantera derecha\",\n",
    "    50: \"Ventanilla delantera izquierda\",\n",
    "    51: \"Ventanilla trasera derecha\",\n",
    "    52: \"Ventanilla trasera izquierda\"\n",
    "}\n",
    "\n",
    "# Diccionario para Tipos de Daño (completo)\n",
    "label_to_cls_danos = {\n",
    "    1: \"Abolladura\",\n",
    "    2: \"Deformación\",\n",
    "    3: \"Desprendimiento\",\n",
    "    4: \"Fractura\",\n",
    "    5: \"Rayón\",\n",
    "    6: \"Rotura\"\n",
    "}\n",
    "\n",
    "# Diccionario para Sugerencia (completo)\n",
    "label_to_cls_sugerencia = {\n",
    "    1: \"Reparar\",\n",
    "    2: \"Reemplazar\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4008651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# Cargar los datasets\n",
    "# =============================================\n",
    "multi_train = pd.read_csv('data/fotos_siniestros/datasets/multi_train.csv', sep='|')\n",
    "multi_val = pd.read_csv('data/fotos_siniestros/datasets/multi_val.csv', sep='|')\n",
    "multi_test = pd.read_csv('data/fotos_siniestros/datasets/multi_test.csv', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59dfe70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# Convertir string a lista\n",
    "# =============================================\n",
    "def convert_string_lists(df):\n",
    "    # Aplicar literal_eval a las columnas relevantes\n",
    "    df['partes'] = df['partes'].apply(ast.literal_eval)\n",
    "    df['dannos'] = df['dannos'].apply(ast.literal_eval)\n",
    "    df['sugerencias'] = df['sugerencias'].apply(ast.literal_eval)\n",
    "    return df\n",
    "# Aplicar a todos tus datasets\n",
    "multi_train = convert_string_lists(multi_train)\n",
    "multi_val = convert_string_lists(multi_val)\n",
    "multi_test = convert_string_lists(multi_test)\n",
    "# Rename columns to match dataset class expectations\n",
    "multi_train = multi_train[['Imagen', 'dannos', 'partes', 'sugerencias']].rename(columns={\n",
    "    'dannos': 'damages',\n",
    "    'partes': 'parts',\n",
    "    'sugerencias': 'suggestions'\n",
    "})\n",
    "# Rename columns to match dataset class expectations\n",
    "multi_val = multi_val[['Imagen', 'dannos', 'partes', 'sugerencias']].rename(columns={\n",
    "    'dannos': 'damages',\n",
    "    'partes': 'parts',\n",
    "    'sugerencias': 'suggestions'\n",
    "})\n",
    "multi_test = multi_test[['Imagen', 'dannos', 'partes', 'sugerencias']].rename(columns={\n",
    "    'dannos': 'damages',\n",
    "    'partes': 'parts',\n",
    "    'sugerencias': 'suggestions'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e718d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# PREPROCESAMIENTO DE DATOS MEJORADO\n",
    "# =============================================\n",
    "def group_rare_labels(labels_list, label_type, min_samples=5):\n",
    "    \"\"\"Agrupa etiquetas raras en una categoría 'Otros'\"\"\"\n",
    "    if label_type == 'parts':\n",
    "        rare_labels = {1, 2, 7, 8, 15, 16, 18, 22, 23, 24, 27, 28, 29, 30, 43, 45, 46, 47, 48, 50, 52}\n",
    "    elif label_type == 'damages':\n",
    "        rare_labels = {3, 4}\n",
    "    else:\n",
    "        return labels_list\n",
    "    \n",
    "    return [label if label not in rare_labels else 999 for label in labels_list]\n",
    "\n",
    "def balance_damage_samples(df):\n",
    "    \"\"\"Sobremuestrea muestras con daños raros\"\"\"\n",
    "    damage_counts = df['damages'].explode().value_counts()\n",
    "    rare_damages = damage_counts[damage_counts < 50].index.tolist()\n",
    "    \n",
    "    rare_samples = df[df['damages'].apply(lambda x: any(d in rare_damages for d in x))]\n",
    "    return pd.concat([df, rare_samples]).reset_index(drop=True)\n",
    "\n",
    "# Aplicar preprocesamiento a los datos\n",
    "if CLASS_GROUPING:\n",
    "    multi_train['parts'] = multi_train['parts'].apply(\n",
    "        lambda x: group_rare_labels(ast.literal_eval(x) if isinstance(x, str) else x, 'parts'))\n",
    "    multi_val['parts'] = multi_val['parts'].apply(\n",
    "        lambda x: group_rare_labels(ast.literal_eval(x) if isinstance(x, str) else x, 'parts'))\n",
    "\n",
    "if BALANCE_DAMAGES:\n",
    "    multi_train = balance_damage_samples(multi_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544d98d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# MODELO CORREGIDO (Versión Final)\n",
    "# =============================================\n",
    "class ImprovedMultiLabelDamageClassifier(nn.Module):\n",
    "    def __init__(self, num_parts, num_damages, num_suggestions):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Cargar ResNet50 sin las capas finales\n",
    "        self.base_model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        \n",
    "        # 2. Extraer todas las capas excepto avgpool y fc\n",
    "        self.feature_extractor = nn.Sequential(*list(self.base_model.children())[:-2])\n",
    "        \n",
    "        # 3. Añadir nuestra propia capa de pooling\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # 4. Configurar capas trainables\n",
    "        # Congelar las primeras capas (hasta layer3)\n",
    "        for name, param in self.feature_extractor.named_parameters():\n",
    "            if 'layer4' not in name:\n",
    "                param.requires_grad = False\n",
    "                \n",
    "        # 5. Capas compartidas\n",
    "        self.shared_fc = nn.Sequential(\n",
    "            nn.Linear(2048, 1024),  # ResNet50 sin fc tiene 2048 features\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        \n",
    "        # 6. Cabezales de clasificación (igual que antes)\n",
    "        self.parts_head = nn.Sequential(\n",
    "            nn.Linear(1024, num_parts)\n",
    "        )\n",
    "        \n",
    "        self.damages_head = nn.Sequential(\n",
    "            nn.Linear(1024, num_damages)\n",
    "        )\n",
    "        \n",
    "        self.suggestions_head = nn.Sequential(\n",
    "            nn.Linear(1024, num_suggestions)\n",
    "        )\n",
    "        \n",
    "        # 7. Inicialización de pesos\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extracción de características\n",
    "        features = self.feature_extractor(x)\n",
    "        features = self.avgpool(features)\n",
    "        features = torch.flatten(features, 1)\n",
    "        \n",
    "        # Verificación de dimensiones\n",
    "        assert features.shape[1] == 2048, f\"Error: Expected 2048 features, got {features.shape[1]}\"\n",
    "        \n",
    "        # Capas compartidas\n",
    "        shared = self.shared_fc(features)\n",
    "        \n",
    "        # Cabezales de salida\n",
    "        return {\n",
    "            'parts': self.parts_head(shared),\n",
    "            'damages': self.damages_head(shared),\n",
    "            'suggestions': self.suggestions_head(shared)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "04c5192e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# FUNCIÓN DE PÉRDIDA MEJORADA\n",
    "# =============================================\n",
    "class AdaptiveFocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.75, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, inputs, targets, weights=None):\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        \n",
    "        # Alpha balance dinámico\n",
    "        alpha_factor = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
    "        focal_loss = alpha_factor * (1-pt)**self.gamma * BCE_loss\n",
    "        \n",
    "        if weights is not None:\n",
    "            # Aplicar pesos por clase con expansión automática\n",
    "            focal_loss = focal_loss * weights.expand_as(targets)\n",
    "            \n",
    "        return focal_loss.mean()\n",
    "\n",
    "def balanced_multi_task_loss(outputs, targets, weights):\n",
    "    # Pérdida para partes con focal loss adaptativo\n",
    "    parts_loss = AdaptiveFocalLoss(alpha=0.75, gamma=2.0)(\n",
    "        outputs['parts'], \n",
    "        targets['parts'].float(),\n",
    "        weights=weights['parts'] if CLASS_WEIGHTS else None\n",
    "    )\n",
    "    \n",
    "    # Pérdida para daños con focal loss más agresivo\n",
    "    damages_loss = AdaptiveFocalLoss(alpha=0.8, gamma=3.0)(\n",
    "        outputs['damages'],\n",
    "        targets['damages'].float(),\n",
    "        weights=weights['damages'] if CLASS_WEIGHTS else None\n",
    "    )\n",
    "    \n",
    "    # Pérdida para sugerencias con label smoothing\n",
    "    suggestions_loss = F.cross_entropy(\n",
    "        outputs['suggestions'],\n",
    "        targets['suggestions'].argmax(dim=1),\n",
    "        weight=weights['suggestions'] if CLASS_WEIGHTS else None,\n",
    "        label_smoothing=0.2\n",
    "    )\n",
    "    \n",
    "    # Balance dinámico basado en el desempeño\n",
    "    return 0.5 * parts_loss + 0.3 * damages_loss + 0.2 * suggestions_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9d14a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# DATASET\n",
    "# =============================================\n",
    "class BalancedMultiLabelDamageDataset(Dataset):\n",
    "    def __init__(self, dataframe, img_dir, transform=None):\n",
    "        self.data = dataframe\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # Convertir strings a listas\n",
    "        for col in ['parts', 'damages', 'suggestions']:\n",
    "            self.data[col] = self.data[col].apply(\n",
    "                lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "            \n",
    "        # Mapeo de clases\n",
    "        self.part_to_idx = {part: idx for idx, part in label_to_cls_piezas.items()}\n",
    "        self.damage_to_idx = {damage: idx for idx, damage in label_to_cls_danos.items()}\n",
    "        self.suggestion_to_idx = {sug: idx for idx, sug in label_to_cls_sugerencia.items()}\n",
    "\n",
    "        # Binarizadores mejorados\n",
    "        self.part_binarizer = MultiLabelBinarizer(classes=sorted(self.part_to_idx.values()))\n",
    "        self.damage_binarizer = MultiLabelBinarizer(classes=sorted(self.damage_to_idx.values()))\n",
    "        self.suggestion_binarizer = MultiLabelBinarizer(classes=sorted(self.suggestion_to_idx.values()))\n",
    "        \n",
    "        # Calcular pesos mejorados\n",
    "        self.part_weights = self._calculate_weights('part')\n",
    "        self.damage_weights = self._calculate_weights('damage')\n",
    "        self.suggestion_weights = self._calculate_weights('suggestion')\n",
    "\n",
    "    def _calculate_weights(self, task):\n",
    "        \"\"\"Versión mejorada con square root inverse frequency\"\"\"\n",
    "        all_labels = [label for labels in self.data[f'{task}s'] \n",
    "                     for label in labels if label in getattr(self, f'{task}_to_idx')]  \n",
    "        if not all_labels:\n",
    "            return torch.ones(len(getattr(self, f'{task}_to_idx')), dtype=torch.float32).to(DEVICE)\n",
    "        counts = Counter(all_labels)\n",
    "        total = len(all_labels)\n",
    "        weights = {cls: 1/np.sqrt(count/total) for cls, count in counts.items()}\n",
    "\n",
    "        # Peso mínimo de 1.0 para clases no presentes\n",
    "        return torch.tensor([weights.get(cls, 1.0) for cls in sorted(getattr(self, f'{task}_to_idx').values())], \n",
    "                          dtype=torch.float32).to(DEVICE)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = \"\"  # Inicializamos la variable     \n",
    "        try:\n",
    "            # Verificamos que el índice sea válido\n",
    "            if idx >= len(self.data):\n",
    "                raise IndexError(f\"Índice {idx} fuera de rango (tamaño del dataset: {len(self.data)})\")\n",
    "                \n",
    "            img_path = os.path.join(self.img_dir, self.data.iloc[idx]['Imagen'])\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error al cargar imagen (índice {idx}): {str(e)}\")\n",
    "            if not img_path:  # Si no se pudo obtener la ruta\n",
    "                img_path = f\"Índice inválido: {idx}\"\n",
    "            print(f\"Ruta problemática: {img_path}\")\n",
    "            image = torch.zeros(3, 224, 224)  # Imagen dummy   \n",
    "\n",
    "        # Resto del código para procesar etiquetas...\n",
    "        parts = torch.zeros(len(self.part_to_idx))\n",
    "        for part in self.data.iloc[idx]['parts']:\n",
    "            if part in self.part_to_idx:\n",
    "                parts[self.part_to_idx[part]] = 1    \n",
    "\n",
    "        damages = torch.zeros(len(self.damage_to_idx))\n",
    "        for damage in self.data.iloc[idx]['damages']:\n",
    "            if damage in self.damage_to_idx:\n",
    "                damages[self.damage_to_idx[damage]] = 1    \n",
    "\n",
    "        suggestions = torch.zeros(len(self.suggestion_to_idx))\n",
    "        for sug in self.data.iloc[idx]['suggestions']:\n",
    "            if sug in self.suggestion_to_idx:\n",
    "                suggestions[self.suggestion_to_idx[sug]] = 1\n",
    "                \n",
    "        if self.transform:\n",
    "            image = self.transform(image)    \n",
    "        return image, {\n",
    "            'parts': parts,\n",
    "            'damages': damages,\n",
    "            'suggestions': suggestions\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd034a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# DATASET MEJORADO\n",
    "# =============================================\n",
    "class EnhancedDamageDataset(BalancedMultiLabelDamageDataset):\n",
    "    def _calculate_weights(self, task):\n",
    "        \"\"\"Versión mejorada con suavizado más agresivo para clases raras\"\"\"\n",
    "        all_labels = [label for labels in self.data[f'{task}s'] \n",
    "                     for label in labels if label in getattr(self, f'{task}_to_idx')]\n",
    "        \n",
    "        if not all_labels:\n",
    "            return torch.ones(len(getattr(self, f'{task}_to_idx')), dtype=torch.float32).to(DEVICE)\n",
    "        \n",
    "        counts = Counter(all_labels)\n",
    "        total = len(all_labels)\n",
    "        \n",
    "        # Suavizado más fuerte para clases raras\n",
    "        weights = {\n",
    "            cls: 1/(count/total)**0.7 if count > MIN_SAMPLES_CLASS \n",
    "            else 1/(MIN_SAMPLES_CLASS/total)**0.7\n",
    "            for cls, count in counts.items()\n",
    "        }\n",
    "        \n",
    "        # Asegurar peso mínimo para clases no vistas\n",
    "        return torch.tensor(\n",
    "            [weights.get(cls, 1.0) for cls in sorted(getattr(self, f'{task}_to_idx').values())],\n",
    "            dtype=torch.float32\n",
    "        ).to(DEVICE)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image, targets = super().__getitem__(idx)\n",
    "        \n",
    "        # Aumentación adicional para muestras raras\n",
    "        if self.transform and any(\n",
    "            label in {1, 2, 7, 8, 15, 16, 18, 22, 23, 24, 27, 28, 29, 30, 43, 45, 46, 47, 48, 50, 52}\n",
    "            for label in self.data.iloc[idx]['parts']\n",
    "        ):\n",
    "            # Aplicar transformaciones extra\n",
    "            image = transforms.functional.adjust_brightness(image, 1.2)\n",
    "            image = transforms.functional.adjust_contrast(image, 1.2)\n",
    "        \n",
    "        return image, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fec1099b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# DATA AUGMENTATION\n",
    "# =============================================\n",
    "def get_transforms():\n",
    "    # Transformaciones base\n",
    "    base_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    if AUGMENTATION:\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.RandomAffine(0, shear=15),\n",
    "            transforms.RandomPerspective(distortion_scale=0.2, p=0.3),\n",
    "            transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.3),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    else:\n",
    "        train_transform = base_transform\n",
    "    return {\n",
    "        'train': train_transform,\n",
    "        'val': base_transform,\n",
    "        'test': base_transform\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9c2a993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# Recolecta las imagenes\n",
    "# =============================================\n",
    "def collate_fn(batch):\n",
    "    # Filtrar None (imágenes que fallaron al cargar)\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    \n",
    "    # Si todo el batch falló, retornar un batch dummy\n",
    "    if len(batch) == 0:\n",
    "        dummy_image = torch.zeros(3, 224, 224)\n",
    "        dummy_target = {\n",
    "            'parts': torch.zeros(len(label_to_cls_piezas)),\n",
    "            'damages': torch.zeros(len(label_to_cls_danos)),\n",
    "            'suggestions': torch.zeros(len(label_to_cls_sugerencia))\n",
    "        }\n",
    "        return dummy_image.unsqueeze(0), dummy_target\n",
    "    \n",
    "    # Procesamiento normal\n",
    "    images = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "    images = torch.stack(images, dim=0)\n",
    "    batch_targets = {\n",
    "        'parts': torch.stack([t['parts'] for t in targets], dim=0),\n",
    "        'damages': torch.stack([t['damages'] for t in targets], dim=0),\n",
    "        'suggestions': torch.stack([t['suggestions'] for t in targets], dim=0)\n",
    "    }\n",
    "    return images, batch_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c62fae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# CLASE EARLY STOPPING\n",
    "# =============================================\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10, delta=0.001, warmup=20):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.warmup = warmup\n",
    "        self.counter = 0\n",
    "        self.best_metric = None\n",
    "        self.early_stop = False\n",
    "        self.epoch = 0\n",
    "    def __call__(self, current_metric):\n",
    "        self.epoch += 1\n",
    "        if self.epoch < self.warmup:  # Periodo de calentamiento\n",
    "            return False\n",
    "        if self.best_metric is None:\n",
    "            self.best_metric = current_metric\n",
    "        elif current_metric < self.best_metric + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_metric = current_metric\n",
    "            self.counter = 0\n",
    "        return self.early_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e312644d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# FUNCIONES AUXILIARES PARA EL ENTRENAMIENTO\n",
    "# =============================================\n",
    "def log_metrics(epoch, train_loss, val_metrics, writer=None):\n",
    "    \"\"\"Registra las métricas de entrenamiento y validación\"\"\"\n",
    "    combined_metric = 0.5*val_metrics['parts']['f1_macro'] + 0.3*val_metrics['damages']['f1_macro'] + 0.2*val_metrics['suggestions']['f1_macro']\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1} Metrics:\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Combined Validation Metric: {combined_metric:.4f}\")\n",
    "    print(\"Detailed Validation Metrics:\")\n",
    "    for task, metrics in val_metrics.items():\n",
    "        print(f\"  {task.capitalize():12} - Acc: {metrics['accuracy']:.4f} | F1: {metrics['f1_macro']:.4f}\")\n",
    "    \n",
    "    if writer:\n",
    "        writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "        writer.add_scalar('Metric/combined', combined_metric, epoch)\n",
    "        for task, metrics in val_metrics.items():\n",
    "            writer.add_scalar(f'Accuracy/{task}', metrics['accuracy'], epoch)\n",
    "            writer.add_scalar(f'F1/{task}', metrics['f1_macro'], epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8cae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# ESTRATEGIA DE ENTRENAMIENTO MEJORADA\n",
    "# =============================================\n",
    "def train_enhanced():\n",
    "    # 1. Configuración inicial\n",
    "    writer = SummaryWriter() if USE_TENSORBOARD else None\n",
    "    data_transforms = get_transforms()\n",
    "    \n",
    "    # 2. Crear datasets mejorados\n",
    "    train_dataset = EnhancedDamageDataset(multi_train, '../data/fotos_siniestros/', data_transforms['train'])\n",
    "    val_dataset = EnhancedDamageDataset(multi_val, '../data/fotos_siniestros/', data_transforms['val'])\n",
    "    \n",
    "    # 3. Sampler balanceado para sugerencias\n",
    "    def get_balanced_sampler(dataset):\n",
    "        suggestion_labels = [labels[0] if len(labels) > 0 else 1 \n",
    "                           for labels in dataset.data['suggestions']]\n",
    "        class_counts = torch.bincount(torch.tensor(suggestion_labels))\n",
    "        class_weights = 1. / class_counts.float()\n",
    "        sample_weights = class_weights[torch.tensor(suggestion_labels)]\n",
    "        return WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
    "    \n",
    "    # 4. DataLoaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        sampler=get_balanced_sampler(train_dataset),\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=4,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # 5. Inicializar modelo mejorado\n",
    "    model = ImprovedMultiLabelDamageClassifier(\n",
    "        num_parts=len(label_to_cls_piezas),\n",
    "        num_damages=len(label_to_cls_danos),\n",
    "        num_suggestions=len(label_to_cls_sugerencia)\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    # 6. Optimizador con learning rates diferenciados\n",
    "    optimizer = optim.AdamW([\n",
    "        {'params': model.parts_head.parameters(), 'lr': 1e-3},\n",
    "        {'params': model.damages_head.parameters(), 'lr': 1e-3},\n",
    "        {'params': model.suggestions_head.parameters(), 'lr': 1e-3},\n",
    "        {'params': model.shared_features.parameters(), 'lr': 1e-4},\n",
    "        {'params': model.base_model.parameters(), 'lr': 1e-5}\n",
    "    ], weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "    \n",
    "    # 7. Early stopping mejorado\n",
    "    early_stopper = EarlyStopping(patience=PATIENCE, delta=0.001, warmup=20)\n",
    "    \n",
    "    # 8. Entrenamiento por fases\n",
    "    best_metric = 0\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        # Fase 1: Solo cabezales (primeras 15 épocas)\n",
    "        if epoch == 15:\n",
    "            for param in model.base_model.layer3.parameters():\n",
    "                param.requires_grad = True\n",
    "            print(\"\\nDescongelando capas layer3\")\n",
    "        \n",
    "        # Fase 2: Fine-tuning completo (después de 30 épocas)\n",
    "        if epoch == 30:\n",
    "            for param in model.base_model.parameters():\n",
    "                param.requires_grad = True\n",
    "            print(\"\\nDescongelando todo el modelo\")\n",
    "\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS}')\n",
    "        \n",
    "        for inputs, targets in progress_bar:\n",
    "            inputs = inputs.to(DEVICE, non_blocking=True)\n",
    "            targets = {k: v.to(DEVICE, non_blocking=True) for k, v in targets.items()}\n",
    "            \n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            loss = balanced_multi_task_loss(outputs, targets, {\n",
    "                'parts': train_dataset.part_weights,\n",
    "                'damages': train_dataset.damage_weights,\n",
    "                'suggestions': train_dataset.suggestion_weights\n",
    "            })\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())    \n",
    "        \n",
    "        # Validación con umbrales adaptativos\n",
    "        val_metrics = evaluate_with_adaptive_thresholds(model, val_loader)\n",
    "        current_metric = 0.5*val_metrics['parts']['f1_macro'] + 0.3*val_metrics['damages']['f1_macro'] + 0.2*val_metrics['suggestions']['f1_macro']\n",
    "        scheduler.step(current_metric)\n",
    "        #############################################\n",
    "        # MONITOREO DEL LEARNING RATE\n",
    "        print(f\"\\nCurrent Learning Rate: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        #############################################\n",
    "        \n",
    "        # Early stopping\n",
    "        if EARLY_STOPPING and early_stopper(current_metric):\n",
    "            print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        # Guardar mejor modelo\n",
    "        if current_metric > best_metric:\n",
    "            best_metric = current_metric\n",
    "            torch.save(model.state_dict(), 'best_enhanced_model.pth')\n",
    "            print(f\"\\nMejor modelo guardado con métrica: {best_metric:.4f}\")\n",
    "        \n",
    "        # Logging mejorado\n",
    "        log_metrics(epoch, epoch_loss/len(train_loader), val_metrics, writer)\n",
    "        \n",
    "        # Evaluación detallada cada 5 épocas\n",
    "        if epoch % 5 == 0:\n",
    "            detailed_class_evaluation(model, val_loader, label_to_cls_piezas, label_to_cls_danos)\n",
    "    \n",
    "    if writer:\n",
    "        writer.close()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0d406066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# FUNCIONES AUXILIARES MEJORADAS\n",
    "# =============================================\n",
    "def evaluate_with_adaptive_thresholds(model, data_loader):\n",
    "    \"\"\"Evaluación con umbrales optimizados por clase\"\"\"\n",
    "    model.eval()\n",
    "    parts_probs, parts_targets = [], []\n",
    "    damages_probs, damages_targets = [], []\n",
    "    suggestions_preds, suggestions_targets = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            parts_probs.append(torch.sigmoid(outputs['parts']).cpu())\n",
    "            parts_targets.append(targets['parts'].cpu())\n",
    "            \n",
    "            damages_probs.append(torch.sigmoid(outputs['damages']).cpu())\n",
    "            damages_targets.append(targets['damages'].cpu())\n",
    "            \n",
    "            suggestions_preds.append(torch.softmax(outputs['suggestions'], dim=1).cpu())\n",
    "            suggestions_targets.append(targets['suggestions'].cpu())\n",
    "    \n",
    "    # Convertir a tensores\n",
    "    parts_probs = torch.cat(parts_probs)\n",
    "    parts_targets = torch.cat(parts_targets)\n",
    "    damages_probs = torch.cat(damages_probs)\n",
    "    damages_targets = torch.cat(damages_targets)\n",
    "    suggestions_preds = torch.cat(suggestions_preds)\n",
    "    suggestions_targets = torch.cat(suggestions_targets)\n",
    "    \n",
    "    # Encontrar umbrales óptimos\n",
    "    parts_threshold = find_optimal_threshold(parts_probs, parts_targets)\n",
    "    damages_threshold = find_optimal_threshold(damages_probs, damages_targets)\n",
    "    \n",
    "    ######################################################\n",
    "    # DIAGNÓSTICO: Imprimir los umbrales óptimos encontrados\n",
    "    print(f\"\\nUmbral óptimo para partes: {parts_threshold:.4f}\")\n",
    "    print(f\"Umbral óptimo para daños: {damages_threshold:.4f}\")\n",
    "    # if epoch % 5 == 0:  # Mostrar cada 5 épocas\n",
    "    #     print(f\"\\nUmbral óptimo para partes: {parts_threshold:.4f}\")\n",
    "    #     print(f\"Umbral óptimo para daños: {damages_threshold:.4f}\")\n",
    "    # Guardar estos umbrales en un archivo de log\n",
    "    with open(\"reportes/training_log.txt\", \"a\") as f:\n",
    "        f.write(f\"Parts threshold={parts_threshold:.4f}, Damages threshold={damages_threshold:.4f}\\n\")\n",
    "    ######################################################\n",
    "    \n",
    "    # Calcular métricas con umbrales óptimos\n",
    "    def calculate_metrics(preds, targets, threshold):\n",
    "        preds = (preds > threshold).float()\n",
    "        accuracy = accuracy_score(targets, preds)\n",
    "        f1 = f1_score(targets, preds, average='macro', zero_division=0)\n",
    "        return {'accuracy': accuracy, 'f1_macro': f1, 'threshold': threshold}\n",
    "    \n",
    "    # Evaluación de sugerencias (clasificación multi-clase)\n",
    "    suggestions_preds = suggestions_preds.argmax(dim=1)\n",
    "    suggestions_targets = suggestions_targets.argmax(dim=1)\n",
    "    suggestions_accuracy = accuracy_score(suggestions_targets, suggestions_preds)\n",
    "    suggestions_f1 = f1_score(suggestions_targets, suggestions_preds, average='macro', zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'parts': calculate_metrics(parts_probs, parts_targets, parts_threshold),\n",
    "        'damages': calculate_metrics(damages_probs, damages_targets, damages_threshold),\n",
    "        'suggestions': {\n",
    "            'accuracy': suggestions_accuracy,\n",
    "            'f1_macro': suggestions_f1,\n",
    "            'threshold': 0.5  # No aplica para clasificación multi-clase\n",
    "        }\n",
    "    }\n",
    "\n",
    "def find_optimal_threshold(probs, targets, num_thresholds=20):\n",
    "    \"\"\"Encuentra el mejor threshold para maximizar F1\"\"\"\n",
    "    best_threshold = 0.5\n",
    "    best_f1 = 0\n",
    "    \n",
    "    for threshold in np.linspace(0.1, 0.9, num_thresholds):\n",
    "        preds = (probs > threshold).float()\n",
    "        current_f1 = f1_score(targets, preds, average='macro', zero_division=0)\n",
    "        \n",
    "        if current_f1 > best_f1:\n",
    "            best_f1 = current_f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    return best_threshold\n",
    "\n",
    "def detailed_class_evaluation(model, data_loader, parts_dict, damages_dict):\n",
    "    \"\"\"Evalúa el desempeño por clase individual\"\"\"\n",
    "    model.eval()\n",
    "    parts_results = {name: {'tp': 0, 'fp': 0, 'fn': 0} for name in parts_dict.values()}\n",
    "    damages_results = {name: {'tp': 0, 'fp': 0, 'fn': 0} for name in damages_dict.values()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Partes\n",
    "            parts_preds = torch.sigmoid(outputs['parts']) > 0.3\n",
    "            for i in range(parts_preds.shape[1]):\n",
    "                pred = parts_preds[:, i]\n",
    "                true = targets['parts'][:, i].bool()\n",
    "                \n",
    "                name = parts_dict[i+1]\n",
    "                parts_results[name]['tp'] += (pred & true).sum().item()\n",
    "                parts_results[name]['fp'] += (pred & ~true).sum().item()\n",
    "                parts_results[name]['fn'] += (~pred & true).sum().item()\n",
    "            \n",
    "            # Daños\n",
    "            damages_preds = torch.sigmoid(outputs['damages']) > 0.2\n",
    "            for i in range(damages_preds.shape[1]):\n",
    "                pred = damages_preds[:, i]\n",
    "                true = targets['damages'][:, i].bool()\n",
    "                \n",
    "                name = damages_dict[i+1]\n",
    "                damages_results[name]['tp'] += (pred & true).sum().item()\n",
    "                damages_results[name]['fp'] += (pred & ~true).sum().item()\n",
    "                damages_results[name]['fn'] += (~pred & true).sum().item()\n",
    "    \n",
    "    # Calcular métricas por clase\n",
    "    print(\"\\nEvaluación Detallada por Clase:\")\n",
    "    print(\"\\nPartes:\")\n",
    "    for name, stats in parts_results.items():\n",
    "        precision = stats['tp'] / (stats['tp'] + stats['fp'] + 1e-10)\n",
    "        recall = stats['tp'] / (stats['tp'] + stats['fn'] + 1e-10)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "        print(f\"{name:30} - Prec: {precision:.2f}, Rec: {recall:.2f}, F1: {f1:.2f}\")\n",
    "    \n",
    "    print(\"\\nDaños:\")\n",
    "    for name, stats in damages_results.items():\n",
    "        precision = stats['tp'] / (stats['tp'] + stats['fp'] + 1e-10)\n",
    "        recall = stats['tp'] / (stats['tp'] + stats['fn'] + 1e-10)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "        print(f\"{name:30} - Prec: {precision:.2f}, Rec: {recall:.2f}, F1: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0ec556",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 11/11 [01:30<00:00,  8.27s/it, loss=1.96]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Umbral óptimo para partes: 0.5000\n",
      "Umbral óptimo para daños: 0.5000\n",
      "\n",
      "Current Learning Rate: 1.00e-03\n",
      "\n",
      "Mejor modelo guardado con métrica: 0.0630\n",
      "\n",
      "Epoch 1 Metrics:\n",
      "Train Loss: 2.3834\n",
      "Combined Validation Metric: 0.0630\n",
      "Detailed Validation Metrics:\n",
      "  Parts        - Acc: 0.0000 | F1: 0.0000\n",
      "  Damages      - Acc: 0.0000 | F1: 0.0000\n",
      "  Suggestions  - Acc: 0.4599 | F1: 0.3150\n",
      "\n",
      "Evaluación Detallada por Clase:\n",
      "\n",
      "Partes:\n",
      "Antiniebla delantero derecho   - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Antiniebla delantero izquierdo - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Capó                           - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Cerradura capo                 - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Cerradura maletero             - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Cerradura puerta               - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Espejo lateral derecho         - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Espejo lateral izquierdo       - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Faros derecho                  - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Faros izquierdo                - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Guardabarros delantero derecho - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Guardabarros delantero izquierdo - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Guardabarros trasero derecho   - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Guardabarros trasero izquierdo - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Luz indicadora delantera derecha - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Luz indicadora delantera izquierda - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Luz indicadora trasera derecha - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Luz indicadora trasera izquierda - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Luz trasera derecho            - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Luz trasera izquierdo          - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Maletero                       - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Manija derecha                 - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Manija izquierda               - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Marco de la ventana            - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Marco de las puertas           - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Moldura capó                   - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Moldura puerta delantera derecha - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Moldura puerta delantera izquierda - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Moldura puerta trasera derecha - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Moldura puerta trasera izquierda - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Parabrisas delantero           - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Parabrisas trasero             - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Parachoques delantero          - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Parachoques trasero            - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Puerta delantera derecha       - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Puerta delantera izquierda     - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Puerta trasera derecha         - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Puerta trasera izquierda       - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Rejilla, parrilla              - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Rueda                          - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Tapa de combustible            - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Tapa de rueda                  - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Techo                          - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Techo corredizo                - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Ventana delantera derecha      - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Ventana delantera izquierda    - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Ventana trasera derecha        - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Ventana trasera izquierda      - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Ventanilla delantera derecha   - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Ventanilla delantera izquierda - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Ventanilla trasera derecha     - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Ventanilla trasera izquierda   - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "\n",
      "Daños:\n",
      "Abolladura                     - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Deformación                    - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Desprendimiento                - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Fractura                       - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Rayón                          - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Rotura                         - Prec: 0.00, Rec: 0.00, F1: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: 100%|██████████| 11/11 [01:40<00:00,  9.18s/it, loss=1.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Umbral óptimo para partes: 0.5000\n",
      "Umbral óptimo para daños: 0.5000\n",
      "\n",
      "Current Learning Rate: 1.00e-03\n",
      "\n",
      "Mejor modelo guardado con métrica: 0.0836\n",
      "\n",
      "Epoch 2 Metrics:\n",
      "Train Loss: 1.4412\n",
      "Combined Validation Metric: 0.0836\n",
      "Detailed Validation Metrics:\n",
      "  Parts        - Acc: 0.1916 | F1: 0.0000\n",
      "  Damages      - Acc: 0.0732 | F1: 0.0000\n",
      "  Suggestions  - Acc: 0.7178 | F1: 0.4178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100: 100%|██████████| 11/11 [01:44<00:00,  9.53s/it, loss=0.781]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Umbral óptimo para partes: 0.5000\n",
      "Umbral óptimo para daños: 0.5000\n",
      "\n",
      "Current Learning Rate: 1.00e-03\n",
      "\n",
      "Mejor modelo guardado con métrica: 0.0927\n",
      "\n",
      "Epoch 3 Metrics:\n",
      "Train Loss: 0.8927\n",
      "Combined Validation Metric: 0.0927\n",
      "Detailed Validation Metrics:\n",
      "  Parts        - Acc: 0.7491 | F1: 0.0000\n",
      "  Damages      - Acc: 0.2683 | F1: 0.0000\n",
      "  Suggestions  - Acc: 0.8641 | F1: 0.4636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: 100%|██████████| 11/11 [01:35<00:00,  8.66s/it, loss=0.443]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Umbral óptimo para partes: 0.5000\n",
      "Umbral óptimo para daños: 0.5000\n",
      "\n",
      "Current Learning Rate: 1.00e-03\n",
      "\n",
      "Mejor modelo guardado con métrica: 0.0973\n",
      "\n",
      "Epoch 4 Metrics:\n",
      "Train Loss: 0.5777\n",
      "Combined Validation Metric: 0.0973\n",
      "Detailed Validation Metrics:\n",
      "  Parts        - Acc: 0.9268 | F1: 0.0000\n",
      "  Damages      - Acc: 0.5436 | F1: 0.0000\n",
      "  Suggestions  - Acc: 0.9477 | F1: 0.4866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: 100%|██████████| 11/11 [01:27<00:00,  7.94s/it, loss=0.37] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Umbral óptimo para partes: 0.5000\n",
      "Umbral óptimo para daños: 0.5000\n",
      "\n",
      "Current Learning Rate: 1.00e-03\n",
      "\n",
      "Mejor modelo guardado con métrica: 0.0997\n",
      "\n",
      "Epoch 5 Metrics:\n",
      "Train Loss: 0.4275\n",
      "Combined Validation Metric: 0.0997\n",
      "Detailed Validation Metrics:\n",
      "  Parts        - Acc: 0.9756 | F1: 0.0000\n",
      "  Damages      - Acc: 0.7422 | F1: 0.0000\n",
      "  Suggestions  - Acc: 0.9930 | F1: 0.4983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100: 100%|██████████| 11/11 [01:28<00:00,  8.06s/it, loss=0.264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Umbral óptimo para partes: 0.5000\n",
      "Umbral óptimo para daños: 0.5000\n",
      "\n",
      "Current Learning Rate: 1.00e-03\n",
      "\n",
      "Epoch 6 Metrics:\n",
      "Train Loss: 0.3769\n",
      "Combined Validation Metric: 0.0991\n",
      "Detailed Validation Metrics:\n",
      "  Parts        - Acc: 0.9895 | F1: 0.0000\n",
      "  Damages      - Acc: 0.8502 | F1: 0.0000\n",
      "  Suggestions  - Acc: 0.9826 | F1: 0.4956\n",
      "\n",
      "Evaluación Detallada por Clase:\n",
      "\n",
      "Partes:\n",
      "Antiniebla delantero derecho   - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Antiniebla delantero izquierdo - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Capó                           - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Cerradura capo                 - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Cerradura maletero             - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Cerradura puerta               - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Espejo lateral derecho         - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Espejo lateral izquierdo       - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Faros derecho                  - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Faros izquierdo                - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Guardabarros delantero derecho - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Guardabarros delantero izquierdo - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Guardabarros trasero derecho   - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Guardabarros trasero izquierdo - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Luz indicadora delantera derecha - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Luz indicadora delantera izquierda - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Luz indicadora trasera derecha - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Luz indicadora trasera izquierda - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Luz trasera derecho            - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Luz trasera izquierdo          - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Maletero                       - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Manija derecha                 - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Manija izquierda               - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Marco de la ventana            - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Marco de las puertas           - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Moldura capó                   - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Moldura puerta delantera derecha - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Moldura puerta delantera izquierda - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Moldura puerta trasera derecha - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Moldura puerta trasera izquierda - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Parabrisas delantero           - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Parabrisas trasero             - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Parachoques delantero          - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Parachoques trasero            - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Puerta delantera derecha       - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Puerta delantera izquierda     - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Puerta trasera derecha         - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Puerta trasera izquierda       - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Rejilla, parrilla              - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Rueda                          - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Tapa de combustible            - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Tapa de rueda                  - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Techo                          - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Techo corredizo                - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Ventana delantera derecha      - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Ventana delantera izquierda    - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Ventana trasera derecha        - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Ventana trasera izquierda      - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Ventanilla delantera derecha   - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Ventanilla delantera izquierda - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Ventanilla trasera derecha     - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Ventanilla trasera izquierda   - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "\n",
      "Daños:\n",
      "Abolladura                     - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Deformación                    - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Desprendimiento                - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Fractura                       - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Rayón                          - Prec: 0.00, Rec: 0.00, F1: 0.00\n",
      "Rotura                         - Prec: 0.00, Rec: 0.00, F1: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100: 100%|██████████| 11/11 [01:27<00:00,  7.92s/it, loss=0.349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Umbral óptimo para partes: 0.5000\n",
      "Umbral óptimo para daños: 0.5000\n",
      "\n",
      "Current Learning Rate: 1.00e-03\n",
      "\n",
      "Epoch 7 Metrics:\n",
      "Train Loss: 0.3349\n",
      "Combined Validation Metric: 0.0973\n",
      "Detailed Validation Metrics:\n",
      "  Parts        - Acc: 0.9930 | F1: 0.0000\n",
      "  Damages      - Acc: 0.9129 | F1: 0.0000\n",
      "  Suggestions  - Acc: 0.9477 | F1: 0.4866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100: 100%|██████████| 11/11 [01:25<00:00,  7.78s/it, loss=0.347]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Umbral óptimo para partes: 0.5000\n",
      "Umbral óptimo para daños: 0.5000\n",
      "\n",
      "Current Learning Rate: 1.00e-03\n",
      "\n",
      "Epoch 8 Metrics:\n",
      "Train Loss: 0.3324\n",
      "Combined Validation Metric: 0.0973\n",
      "Detailed Validation Metrics:\n",
      "  Parts        - Acc: 0.9895 | F1: 0.0000\n",
      "  Damages      - Acc: 0.9617 | F1: 0.0000\n",
      "  Suggestions  - Acc: 0.9477 | F1: 0.4866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100: 100%|██████████| 11/11 [01:25<00:00,  7.79s/it, loss=0.361]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Umbral óptimo para partes: 0.5000\n",
      "Umbral óptimo para daños: 0.5000\n",
      "\n",
      "Current Learning Rate: 1.00e-03\n",
      "\n",
      "Epoch 9 Metrics:\n",
      "Train Loss: 0.3100\n",
      "Combined Validation Metric: 0.0988\n",
      "Detailed Validation Metrics:\n",
      "  Parts        - Acc: 0.9965 | F1: 0.0000\n",
      "  Damages      - Acc: 0.9791 | F1: 0.0000\n",
      "  Suggestions  - Acc: 0.9756 | F1: 0.4938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100:  73%|███████▎  | 8/11 [01:05<00:23,  7.92s/it, loss=0.287]"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# EJECUCIÓN PRINCIPAL\n",
    "# =============================================\n",
    "if __name__ == '__main__':\n",
    "    trained_model = train_enhanced()\n",
    "    torch.save(trained_model.state_dict(), 'DetectarDannosPartesSugerenciasUsandoMultiplesEtiquetas_V7.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
